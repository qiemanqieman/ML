{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# from utils.networkHelper import *\n",
    "import math\n",
    "from inspect import isfunction\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange, reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# global varialbes\n",
    "image_size = 64\n",
    "channels = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_epochs = 20\n",
    "timesteps = 1000\n",
    "# device = torch.device('cuda:'+str(torch.cuda.device_count()-1)\n",
    "                    #   if torch.cuda.is_available() else 'cpu')\n",
    "data_path_root = '/data/wumin/dataset/celeba/'\n",
    "bs = 100\n",
    "results_folder = Path(\"./samples/ddpm/celeba\")\n",
    "results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((178,178)),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.CelebA(\n",
    "    root=data_path_root, split='train', transform=transform, download=True)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    save_image(x.add(1).mul(0.5), results_folder/'real.png', nrow=10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    # 暂时没用到\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "\n",
    "def Upsample(dim, dim_out=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "        nn.Conv2d(dim, default(dim_out, dim), 3, padding=1),\n",
    "    )\n",
    "\n",
    "\n",
    "def Downsample(dim, dim_out=None):\n",
    "    # No More Strided Convolutions or Pooling\n",
    "    return nn.Sequential(\n",
    "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n",
    "    )\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(\n",
    "            half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    权重标准化后的卷积模块\n",
    "    https://arxiv.org/abs/1903.10520\n",
    "    weight standardization purportedly works synergistically with group normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\")\n",
    "        var = reduce(weight, \"o ... -> o 1 1 1\",\n",
    "                     partial(torch.var, unbiased=False))\n",
    "        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n",
    "\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            normalized_weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(\n",
    "            dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(\n",
    "                t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(\n",
    "                t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\",\n",
    "                        h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        self_condition=False,\n",
    "        resnet_block_groups=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        # changed to 1 and 0 from 7,3\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Downsample(dim_in, dim_out)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out,\n",
    "                                    time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out,\n",
    "                                    time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_dim = default(out_dim, channels)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond=None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim=1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(\n",
    "        ((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "\n",
    "def quadratic_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
    "\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# define alphas\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "# forward diffusion (using the nice property)\n",
    "\n",
    "\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "    \n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t)\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    # Equation 11 in the paper\n",
    "    # Use our model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Algorithm 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "# Algorithm 2 (including returning all images)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "        img = p_sample(model, img, torch.full(\n",
    "            (b,), i, device=device, dtype=torch.long), i)\n",
    "        imgs.append(img)\n",
    "    return torch.stack(imgs, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def show_result(model, epoch, batches=10, display_step=100, show=False, save=False,  path='result.png'):\n",
    "    # save generated images\n",
    "    ds = display_step\n",
    "    all_images_list = sample(model, image_size=image_size, batch_size=batches, channels=channels)\n",
    "    all_images = rearrange(\n",
    "        all_images_list, 't b c h w -> (b t) c h w').add(1).mul(0.5)\n",
    "    all_images = all_images[torch.arange(ds-1, all_images.shape[0], ds),]\n",
    "    save_image(all_images, str(results_folder /\n",
    "               f'sample_epoch_{epoch}.png'), nrow=timesteps//ds)\n",
    "\n",
    "\n",
    "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "    # display the training losses as epoch increasing\n",
    "    x = range(len(hist['losses']))\n",
    "\n",
    "    y = hist['losses']\n",
    "\n",
    "    plt.plot(x, y, label='Loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=5)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "def train(model, optimizer, epoch, train_hist):\n",
    "    losses = []\n",
    "    for step, (batch, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = batch.shape[0]\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(\"epoch: {} step: {} Loss: {:.5f}\".format(\n",
    "                epoch, step, loss.item()))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_hist['losses'].append(np.mean(losses))\n",
    "    print(\"================> epoch: {} average Loss: {:.5f}\".format(\n",
    "                epoch, train_hist['losses'][-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1628"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    dim=image_size * 3,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ").to(device)\n",
    "\n",
    "# model = torch.load('models/ddpm_celeba.pth').to(device)\n",
    "optimizer = Adam(model.parameters(), lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "epoch: 0 step: 0 Loss: 0.53420\n",
      "epoch: 0 step: 50 Loss: 0.04274\n",
      "epoch: 0 step: 100 Loss: 0.01921\n",
      "epoch: 0 step: 150 Loss: 0.02371\n",
      "epoch: 0 step: 200 Loss: 0.02255\n",
      "epoch: 0 step: 250 Loss: 0.00853\n",
      "epoch: 0 step: 300 Loss: 0.01424\n",
      "epoch: 0 step: 350 Loss: 0.00805\n",
      "epoch: 0 step: 400 Loss: 0.01630\n",
      "epoch: 0 step: 450 Loss: 0.01760\n",
      "epoch: 0 step: 500 Loss: 0.01693\n",
      "epoch: 0 step: 550 Loss: 0.01494\n",
      "epoch: 0 step: 600 Loss: 0.01629\n",
      "epoch: 0 step: 650 Loss: 0.01541\n",
      "epoch: 0 step: 700 Loss: 0.01683\n",
      "epoch: 0 step: 750 Loss: 0.01003\n",
      "epoch: 0 step: 800 Loss: 0.01048\n",
      "epoch: 0 step: 850 Loss: 0.01338\n",
      "epoch: 0 step: 900 Loss: 0.01212\n",
      "epoch: 0 step: 950 Loss: 0.01143\n",
      "epoch: 0 step: 1000 Loss: 0.01063\n",
      "epoch: 0 step: 1050 Loss: 0.01329\n",
      "epoch: 0 step: 1100 Loss: 0.01268\n",
      "epoch: 0 step: 1150 Loss: 0.01115\n",
      "epoch: 0 step: 1200 Loss: 0.00900\n",
      "epoch: 0 step: 1250 Loss: 0.01111\n",
      "epoch: 0 step: 1300 Loss: 0.01104\n",
      "epoch: 0 step: 1350 Loss: 0.01077\n",
      "epoch: 0 step: 1400 Loss: 0.01168\n",
      "epoch: 0 step: 1450 Loss: 0.01119\n",
      "epoch: 0 step: 1500 Loss: 0.01251\n",
      "epoch: 0 step: 1550 Loss: 0.01037\n",
      "epoch: 0 step: 1600 Loss: 0.00840\n",
      "================> epoch: 0 average Loss: 0.01773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd966219d6014ca19e281005629f054e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 0 Loss: 0.01040\n",
      "epoch: 1 step: 50 Loss: 0.00665\n",
      "epoch: 1 step: 100 Loss: 0.00941\n",
      "epoch: 1 step: 150 Loss: 0.01146\n",
      "epoch: 1 step: 200 Loss: 0.01117\n",
      "epoch: 1 step: 250 Loss: 0.01205\n",
      "epoch: 1 step: 300 Loss: 0.00984\n",
      "epoch: 1 step: 350 Loss: 0.00759\n",
      "epoch: 1 step: 400 Loss: 0.00941\n",
      "epoch: 1 step: 450 Loss: 0.00811\n",
      "epoch: 1 step: 500 Loss: 0.00878\n",
      "epoch: 1 step: 550 Loss: 0.00546\n",
      "epoch: 1 step: 600 Loss: 0.00933\n",
      "epoch: 1 step: 650 Loss: 0.01537\n",
      "epoch: 1 step: 700 Loss: 0.00800\n",
      "epoch: 1 step: 750 Loss: 0.00757\n",
      "epoch: 1 step: 800 Loss: 0.01089\n",
      "epoch: 1 step: 850 Loss: 0.01022\n",
      "epoch: 1 step: 900 Loss: 0.00510\n",
      "epoch: 1 step: 950 Loss: 0.01194\n",
      "epoch: 1 step: 1000 Loss: 0.00673\n",
      "epoch: 1 step: 1050 Loss: 0.01009\n",
      "epoch: 1 step: 1100 Loss: 0.01301\n",
      "epoch: 1 step: 1150 Loss: 0.01007\n",
      "epoch: 1 step: 1200 Loss: 0.00990\n",
      "epoch: 1 step: 1250 Loss: 0.00886\n",
      "epoch: 1 step: 1300 Loss: 0.01207\n",
      "epoch: 1 step: 1350 Loss: 0.00964\n",
      "epoch: 1 step: 1400 Loss: 0.01280\n",
      "epoch: 1 step: 1450 Loss: 0.01198\n",
      "epoch: 1 step: 1500 Loss: 0.00959\n",
      "epoch: 1 step: 1550 Loss: 0.00697\n",
      "epoch: 1 step: 1600 Loss: 0.00973\n",
      "================> epoch: 1 average Loss: 0.01013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5cbd7a8b974097bf9266a69486f6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 0 Loss: 0.00876\n",
      "epoch: 2 step: 50 Loss: 0.00548\n",
      "epoch: 2 step: 100 Loss: 0.00671\n",
      "epoch: 2 step: 150 Loss: 0.00742\n",
      "epoch: 2 step: 200 Loss: 0.01011\n",
      "epoch: 2 step: 250 Loss: 0.00833\n",
      "epoch: 2 step: 300 Loss: 0.01218\n",
      "epoch: 2 step: 350 Loss: 0.01073\n",
      "epoch: 2 step: 400 Loss: 0.00756\n",
      "epoch: 2 step: 450 Loss: 0.00910\n",
      "epoch: 2 step: 500 Loss: 0.00581\n",
      "epoch: 2 step: 550 Loss: 0.00663\n",
      "epoch: 2 step: 600 Loss: 0.00656\n",
      "epoch: 2 step: 650 Loss: 0.00922\n",
      "epoch: 2 step: 700 Loss: 0.00887\n",
      "epoch: 2 step: 750 Loss: 0.01297\n",
      "epoch: 2 step: 800 Loss: 0.00862\n",
      "epoch: 2 step: 850 Loss: 0.00649\n",
      "epoch: 2 step: 900 Loss: 0.00768\n",
      "epoch: 2 step: 950 Loss: 0.00893\n",
      "epoch: 2 step: 1000 Loss: 0.00715\n",
      "epoch: 2 step: 1050 Loss: 0.01142\n",
      "epoch: 2 step: 1100 Loss: 0.01011\n",
      "epoch: 2 step: 1150 Loss: 0.00826\n",
      "epoch: 2 step: 1200 Loss: 0.01550\n",
      "epoch: 2 step: 1250 Loss: 0.00563\n",
      "epoch: 2 step: 1300 Loss: 0.00966\n",
      "epoch: 2 step: 1350 Loss: 0.00856\n",
      "epoch: 2 step: 1400 Loss: 0.00787\n",
      "epoch: 2 step: 1450 Loss: 0.00909\n",
      "epoch: 2 step: 1500 Loss: 0.00690\n",
      "epoch: 2 step: 1550 Loss: 0.00554\n",
      "epoch: 2 step: 1600 Loss: 0.01208\n",
      "================> epoch: 2 average Loss: 0.00940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfda322ce4004271ae3cc83b9df61caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 0 Loss: 0.01048\n",
      "epoch: 3 step: 50 Loss: 0.00960\n",
      "epoch: 3 step: 100 Loss: 0.00689\n",
      "epoch: 3 step: 150 Loss: 0.00771\n",
      "epoch: 3 step: 200 Loss: 0.00822\n",
      "epoch: 3 step: 250 Loss: 0.00811\n",
      "epoch: 3 step: 300 Loss: 0.01693\n",
      "epoch: 3 step: 350 Loss: 0.00774\n",
      "epoch: 3 step: 400 Loss: 0.01165\n",
      "epoch: 3 step: 450 Loss: 0.00784\n",
      "epoch: 3 step: 500 Loss: 0.00839\n",
      "epoch: 3 step: 550 Loss: 0.00835\n",
      "epoch: 3 step: 600 Loss: 0.01090\n",
      "epoch: 3 step: 650 Loss: 0.00769\n",
      "epoch: 3 step: 700 Loss: 0.00657\n",
      "epoch: 3 step: 750 Loss: 0.01216\n",
      "epoch: 3 step: 800 Loss: 0.00774\n",
      "epoch: 3 step: 850 Loss: 0.00854\n",
      "epoch: 3 step: 900 Loss: 0.01410\n",
      "epoch: 3 step: 950 Loss: 0.01302\n",
      "epoch: 3 step: 1000 Loss: 0.01165\n",
      "epoch: 3 step: 1050 Loss: 0.01050\n",
      "epoch: 3 step: 1100 Loss: 0.00741\n",
      "epoch: 3 step: 1150 Loss: 0.00712\n",
      "epoch: 3 step: 1200 Loss: 0.00854\n",
      "epoch: 3 step: 1250 Loss: 0.00979\n",
      "epoch: 3 step: 1300 Loss: 0.00693\n",
      "epoch: 3 step: 1350 Loss: 0.00648\n",
      "epoch: 3 step: 1400 Loss: 0.01170\n",
      "epoch: 3 step: 1450 Loss: 0.00824\n",
      "epoch: 3 step: 1500 Loss: 0.00700\n",
      "epoch: 3 step: 1550 Loss: 0.01379\n",
      "epoch: 3 step: 1600 Loss: 0.00663\n",
      "================> epoch: 3 average Loss: 0.00914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3da80f51ab8404d8777dcd9b2c14756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 0 Loss: 0.00660\n",
      "epoch: 4 step: 50 Loss: 0.01092\n",
      "epoch: 4 step: 100 Loss: 0.01110\n",
      "epoch: 4 step: 150 Loss: 0.00967\n",
      "epoch: 4 step: 200 Loss: 0.01075\n",
      "epoch: 4 step: 250 Loss: 0.00970\n",
      "epoch: 4 step: 300 Loss: 0.00937\n",
      "epoch: 4 step: 350 Loss: 0.00664\n",
      "epoch: 4 step: 400 Loss: 0.00933\n",
      "epoch: 4 step: 450 Loss: 0.01085\n",
      "epoch: 4 step: 500 Loss: 0.00835\n",
      "epoch: 4 step: 550 Loss: 0.00471\n",
      "epoch: 4 step: 600 Loss: 0.00760\n",
      "epoch: 4 step: 650 Loss: 0.00674\n",
      "epoch: 4 step: 700 Loss: 0.00702\n",
      "epoch: 4 step: 750 Loss: 0.01057\n",
      "epoch: 4 step: 800 Loss: 0.00668\n",
      "epoch: 4 step: 850 Loss: 0.00874\n",
      "epoch: 4 step: 900 Loss: 0.01183\n",
      "epoch: 4 step: 950 Loss: 0.00793\n",
      "epoch: 4 step: 1000 Loss: 0.00847\n",
      "epoch: 4 step: 1050 Loss: 0.00940\n",
      "epoch: 4 step: 1100 Loss: 0.00574\n",
      "epoch: 4 step: 1150 Loss: 0.01033\n",
      "epoch: 4 step: 1200 Loss: 0.00750\n",
      "epoch: 4 step: 1250 Loss: 0.00886\n",
      "epoch: 4 step: 1300 Loss: 0.00965\n",
      "epoch: 4 step: 1350 Loss: 0.00846\n",
      "epoch: 4 step: 1400 Loss: 0.00776\n",
      "epoch: 4 step: 1450 Loss: 0.00852\n",
      "epoch: 4 step: 1500 Loss: 0.00579\n",
      "epoch: 4 step: 1550 Loss: 0.01004\n",
      "epoch: 4 step: 1600 Loss: 0.00954\n",
      "================> epoch: 4 average Loss: 0.00889\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf58722330944258030e6ff75799400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 0 Loss: 0.00799\n",
      "epoch: 5 step: 50 Loss: 0.00849\n",
      "epoch: 5 step: 100 Loss: 0.00843\n",
      "epoch: 5 step: 150 Loss: 0.00787\n",
      "epoch: 5 step: 200 Loss: 0.00618\n",
      "epoch: 5 step: 250 Loss: 0.00732\n",
      "epoch: 5 step: 300 Loss: 0.00726\n",
      "epoch: 5 step: 350 Loss: 0.00704\n",
      "epoch: 5 step: 400 Loss: 0.01133\n",
      "epoch: 5 step: 450 Loss: 0.00838\n",
      "epoch: 5 step: 500 Loss: 0.01240\n",
      "epoch: 5 step: 550 Loss: 0.00686\n",
      "epoch: 5 step: 600 Loss: 0.00789\n",
      "epoch: 5 step: 650 Loss: 0.00730\n",
      "epoch: 5 step: 700 Loss: 0.00752\n",
      "epoch: 5 step: 750 Loss: 0.00710\n",
      "epoch: 5 step: 800 Loss: 0.00775\n",
      "epoch: 5 step: 850 Loss: 0.00648\n",
      "epoch: 5 step: 900 Loss: 0.00906\n",
      "epoch: 5 step: 950 Loss: 0.00971\n",
      "epoch: 5 step: 1000 Loss: 0.01147\n",
      "epoch: 5 step: 1050 Loss: 0.01135\n",
      "epoch: 5 step: 1100 Loss: 0.00941\n",
      "epoch: 5 step: 1150 Loss: 0.00992\n",
      "epoch: 5 step: 1200 Loss: 0.00780\n",
      "epoch: 5 step: 1250 Loss: 0.00879\n",
      "epoch: 5 step: 1300 Loss: 0.00590\n",
      "epoch: 5 step: 1350 Loss: 0.01503\n",
      "epoch: 5 step: 1400 Loss: 0.01091\n",
      "epoch: 5 step: 1450 Loss: 0.00650\n",
      "epoch: 5 step: 1500 Loss: 0.00970\n",
      "epoch: 5 step: 1550 Loss: 0.01189\n",
      "epoch: 5 step: 1600 Loss: 0.00688\n",
      "================> epoch: 5 average Loss: 0.00877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961dd68c335b4dbe80ed97a84780c268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 0 Loss: 0.00620\n",
      "epoch: 6 step: 50 Loss: 0.00883\n",
      "epoch: 6 step: 100 Loss: 0.00880\n",
      "epoch: 6 step: 150 Loss: 0.00713\n",
      "epoch: 6 step: 200 Loss: 0.00939\n",
      "epoch: 6 step: 250 Loss: 0.00910\n",
      "epoch: 6 step: 300 Loss: 0.00627\n",
      "epoch: 6 step: 350 Loss: 0.01016\n",
      "epoch: 6 step: 400 Loss: 0.00787\n",
      "epoch: 6 step: 450 Loss: 0.01043\n",
      "epoch: 6 step: 500 Loss: 0.00864\n",
      "epoch: 6 step: 550 Loss: 0.00730\n",
      "epoch: 6 step: 600 Loss: 0.00693\n",
      "epoch: 6 step: 650 Loss: 0.00883\n",
      "epoch: 6 step: 700 Loss: 0.01108\n",
      "epoch: 6 step: 750 Loss: 0.00785\n",
      "epoch: 6 step: 800 Loss: 0.01150\n",
      "epoch: 6 step: 850 Loss: 0.01129\n",
      "epoch: 6 step: 900 Loss: 0.00847\n",
      "epoch: 6 step: 950 Loss: 0.01059\n",
      "epoch: 6 step: 1000 Loss: 0.00529\n",
      "epoch: 6 step: 1050 Loss: 0.00949\n",
      "epoch: 6 step: 1100 Loss: 0.00663\n",
      "epoch: 6 step: 1150 Loss: 0.00940\n",
      "epoch: 6 step: 1200 Loss: 0.00875\n",
      "epoch: 6 step: 1250 Loss: 0.00771\n",
      "epoch: 6 step: 1300 Loss: 0.00800\n",
      "epoch: 6 step: 1350 Loss: 0.01013\n",
      "epoch: 6 step: 1400 Loss: 0.01395\n",
      "epoch: 6 step: 1450 Loss: 0.00667\n",
      "epoch: 6 step: 1500 Loss: 0.00678\n",
      "epoch: 6 step: 1550 Loss: 0.00727\n",
      "epoch: 6 step: 1600 Loss: 0.00827\n",
      "================> epoch: 6 average Loss: 0.00860\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bd520c00784a6ebbbd7d03a0608a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 0 Loss: 0.01266\n",
      "epoch: 7 step: 50 Loss: 0.00783\n",
      "epoch: 7 step: 100 Loss: 0.00528\n",
      "epoch: 7 step: 150 Loss: 0.00874\n",
      "epoch: 7 step: 200 Loss: 0.00958\n",
      "epoch: 7 step: 250 Loss: 0.00677\n",
      "epoch: 7 step: 300 Loss: 0.00822\n",
      "epoch: 7 step: 350 Loss: 0.00886\n",
      "epoch: 7 step: 400 Loss: 0.00774\n",
      "epoch: 7 step: 450 Loss: 0.00572\n",
      "epoch: 7 step: 500 Loss: 0.00635\n",
      "epoch: 7 step: 550 Loss: 0.00915\n",
      "epoch: 7 step: 600 Loss: 0.01258\n",
      "epoch: 7 step: 650 Loss: 0.00788\n",
      "epoch: 7 step: 700 Loss: 0.01111\n",
      "epoch: 7 step: 750 Loss: 0.00591\n",
      "epoch: 7 step: 800 Loss: 0.00694\n",
      "epoch: 7 step: 850 Loss: 0.00993\n",
      "epoch: 7 step: 900 Loss: 0.00856\n",
      "epoch: 7 step: 950 Loss: 0.00710\n",
      "epoch: 7 step: 1000 Loss: 0.00950\n",
      "epoch: 7 step: 1050 Loss: 0.00728\n",
      "epoch: 7 step: 1100 Loss: 0.00723\n",
      "epoch: 7 step: 1150 Loss: 0.00925\n",
      "epoch: 7 step: 1200 Loss: 0.00885\n",
      "epoch: 7 step: 1250 Loss: 0.00976\n",
      "epoch: 7 step: 1300 Loss: 0.00646\n",
      "epoch: 7 step: 1350 Loss: 0.00735\n",
      "epoch: 7 step: 1400 Loss: 0.01114\n",
      "epoch: 7 step: 1450 Loss: 0.00756\n",
      "epoch: 7 step: 1500 Loss: 0.01171\n",
      "epoch: 7 step: 1550 Loss: 0.00675\n",
      "epoch: 7 step: 1600 Loss: 0.00699\n",
      "================> epoch: 7 average Loss: 0.00846\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab40106b1d9430194d96f8d67260785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 0 Loss: 0.00807\n",
      "epoch: 8 step: 50 Loss: 0.00827\n",
      "epoch: 8 step: 100 Loss: 0.00524\n",
      "epoch: 8 step: 150 Loss: 0.00752\n",
      "epoch: 8 step: 200 Loss: 0.00685\n",
      "epoch: 8 step: 250 Loss: 0.00890\n",
      "epoch: 8 step: 300 Loss: 0.00846\n",
      "epoch: 8 step: 350 Loss: 0.01104\n",
      "epoch: 8 step: 400 Loss: 0.00521\n",
      "epoch: 8 step: 450 Loss: 0.01499\n",
      "epoch: 8 step: 500 Loss: 0.00921\n",
      "epoch: 8 step: 550 Loss: 0.00467\n",
      "epoch: 8 step: 600 Loss: 0.00750\n",
      "epoch: 8 step: 650 Loss: 0.01163\n",
      "epoch: 8 step: 700 Loss: 0.00525\n",
      "epoch: 8 step: 750 Loss: 0.00962\n",
      "epoch: 8 step: 800 Loss: 0.00864\n",
      "epoch: 8 step: 850 Loss: 0.00802\n",
      "epoch: 8 step: 900 Loss: 0.00919\n",
      "epoch: 8 step: 950 Loss: 0.00923\n",
      "epoch: 8 step: 1000 Loss: 0.00984\n",
      "epoch: 8 step: 1050 Loss: 0.01017\n",
      "epoch: 8 step: 1100 Loss: 0.00780\n",
      "epoch: 8 step: 1150 Loss: 0.00821\n",
      "epoch: 8 step: 1200 Loss: 0.01162\n",
      "epoch: 8 step: 1250 Loss: 0.00855\n",
      "epoch: 8 step: 1300 Loss: 0.00925\n",
      "epoch: 8 step: 1350 Loss: 0.00560\n",
      "epoch: 8 step: 1400 Loss: 0.00673\n",
      "epoch: 8 step: 1450 Loss: 0.00786\n",
      "epoch: 8 step: 1500 Loss: 0.01517\n",
      "epoch: 8 step: 1550 Loss: 0.00800\n",
      "epoch: 8 step: 1600 Loss: 0.00961\n",
      "================> epoch: 8 average Loss: 0.00839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9086d80609d34fa48ddcfc22ddf80b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 0 Loss: 0.00596\n",
      "epoch: 9 step: 50 Loss: 0.00809\n",
      "epoch: 9 step: 100 Loss: 0.00965\n",
      "epoch: 9 step: 150 Loss: 0.00924\n",
      "epoch: 9 step: 200 Loss: 0.00762\n",
      "epoch: 9 step: 250 Loss: 0.00744\n",
      "epoch: 9 step: 300 Loss: 0.01075\n",
      "epoch: 9 step: 350 Loss: 0.00528\n",
      "epoch: 9 step: 400 Loss: 0.00919\n",
      "epoch: 9 step: 450 Loss: 0.00676\n",
      "epoch: 9 step: 500 Loss: 0.00729\n",
      "epoch: 9 step: 550 Loss: 0.00640\n",
      "epoch: 9 step: 600 Loss: 0.01086\n",
      "epoch: 9 step: 650 Loss: 0.00784\n",
      "epoch: 9 step: 700 Loss: 0.01179\n",
      "epoch: 9 step: 750 Loss: 0.01102\n",
      "epoch: 9 step: 800 Loss: 0.00893\n",
      "epoch: 9 step: 850 Loss: 0.00726\n",
      "epoch: 9 step: 900 Loss: 0.00749\n",
      "epoch: 9 step: 950 Loss: 0.01376\n",
      "epoch: 9 step: 1000 Loss: 0.00402\n",
      "epoch: 9 step: 1050 Loss: 0.00893\n",
      "epoch: 9 step: 1100 Loss: 0.00700\n",
      "epoch: 9 step: 1150 Loss: 0.01137\n",
      "epoch: 9 step: 1200 Loss: 0.00611\n",
      "epoch: 9 step: 1250 Loss: 0.00822\n",
      "epoch: 9 step: 1300 Loss: 0.01017\n",
      "epoch: 9 step: 1350 Loss: 0.01015\n",
      "epoch: 9 step: 1400 Loss: 0.00777\n",
      "epoch: 9 step: 1450 Loss: 0.00620\n",
      "epoch: 9 step: 1500 Loss: 0.00839\n",
      "epoch: 9 step: 1550 Loss: 0.00914\n",
      "epoch: 9 step: 1600 Loss: 0.00745\n",
      "================> epoch: 9 average Loss: 0.00818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b8cdae048f4434a496dae0a1ed0d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 0 Loss: 0.00896\n",
      "epoch: 10 step: 50 Loss: 0.00807\n",
      "epoch: 10 step: 100 Loss: 0.00518\n",
      "epoch: 10 step: 150 Loss: 0.00961\n",
      "epoch: 10 step: 200 Loss: 0.00387\n",
      "epoch: 10 step: 250 Loss: 0.00766\n",
      "epoch: 10 step: 300 Loss: 0.01585\n",
      "epoch: 10 step: 350 Loss: 0.00770\n",
      "epoch: 10 step: 400 Loss: 0.00811\n",
      "epoch: 10 step: 450 Loss: 0.00504\n",
      "epoch: 10 step: 500 Loss: 0.00802\n",
      "epoch: 10 step: 550 Loss: 0.00868\n",
      "epoch: 10 step: 600 Loss: 0.00871\n",
      "epoch: 10 step: 650 Loss: 0.00811\n",
      "epoch: 10 step: 700 Loss: 0.00699\n",
      "epoch: 10 step: 750 Loss: 0.01108\n",
      "epoch: 10 step: 800 Loss: 0.00904\n",
      "epoch: 10 step: 850 Loss: 0.00852\n",
      "epoch: 10 step: 900 Loss: 0.00751\n",
      "epoch: 10 step: 950 Loss: 0.00675\n",
      "epoch: 10 step: 1000 Loss: 0.00874\n",
      "epoch: 10 step: 1050 Loss: 0.00911\n",
      "epoch: 10 step: 1100 Loss: 0.00933\n",
      "epoch: 10 step: 1150 Loss: 0.00835\n",
      "epoch: 10 step: 1200 Loss: 0.00772\n",
      "epoch: 10 step: 1250 Loss: 0.00787\n",
      "epoch: 10 step: 1300 Loss: 0.00729\n",
      "epoch: 10 step: 1350 Loss: 0.00857\n",
      "epoch: 10 step: 1400 Loss: 0.00645\n",
      "epoch: 10 step: 1450 Loss: 0.00734\n",
      "epoch: 10 step: 1500 Loss: 0.00845\n",
      "epoch: 10 step: 1550 Loss: 0.00700\n",
      "epoch: 10 step: 1600 Loss: 0.00460\n",
      "================> epoch: 10 average Loss: 0.00813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2fcd026b91428f93204eedc294876f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 0 Loss: 0.01165\n",
      "epoch: 11 step: 50 Loss: 0.00935\n",
      "epoch: 11 step: 100 Loss: 0.00509\n",
      "epoch: 11 step: 150 Loss: 0.00593\n",
      "epoch: 11 step: 200 Loss: 0.00645\n",
      "epoch: 11 step: 250 Loss: 0.01111\n",
      "epoch: 11 step: 300 Loss: 0.00602\n",
      "epoch: 11 step: 350 Loss: 0.00767\n",
      "epoch: 11 step: 400 Loss: 0.00819\n",
      "epoch: 11 step: 450 Loss: 0.00758\n",
      "epoch: 11 step: 500 Loss: 0.00951\n",
      "epoch: 11 step: 550 Loss: 0.00680\n",
      "epoch: 11 step: 600 Loss: 0.00758\n",
      "epoch: 11 step: 650 Loss: 0.00690\n",
      "epoch: 11 step: 700 Loss: 0.00651\n",
      "epoch: 11 step: 750 Loss: 0.00538\n",
      "epoch: 11 step: 800 Loss: 0.00833\n",
      "epoch: 11 step: 850 Loss: 0.00912\n",
      "epoch: 11 step: 900 Loss: 0.00572\n",
      "epoch: 11 step: 950 Loss: 0.00620\n",
      "epoch: 11 step: 1000 Loss: 0.00610\n",
      "epoch: 11 step: 1050 Loss: 0.00721\n",
      "epoch: 11 step: 1100 Loss: 0.00902\n",
      "epoch: 11 step: 1150 Loss: 0.00738\n",
      "epoch: 11 step: 1200 Loss: 0.01115\n",
      "epoch: 11 step: 1250 Loss: 0.00855\n",
      "epoch: 11 step: 1300 Loss: 0.00856\n",
      "epoch: 11 step: 1350 Loss: 0.00880\n",
      "epoch: 11 step: 1400 Loss: 0.00769\n",
      "epoch: 11 step: 1450 Loss: 0.01064\n",
      "epoch: 11 step: 1500 Loss: 0.01370\n",
      "epoch: 11 step: 1550 Loss: 0.00736\n",
      "epoch: 11 step: 1600 Loss: 0.00816\n",
      "================> epoch: 11 average Loss: 0.00806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8687b7760804b9986d1929961fdf33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 0 Loss: 0.00784\n",
      "epoch: 12 step: 50 Loss: 0.00930\n",
      "epoch: 12 step: 100 Loss: 0.00868\n",
      "epoch: 12 step: 150 Loss: 0.00670\n",
      "epoch: 12 step: 200 Loss: 0.00641\n",
      "epoch: 12 step: 250 Loss: 0.00676\n",
      "epoch: 12 step: 300 Loss: 0.00610\n",
      "epoch: 12 step: 350 Loss: 0.00993\n",
      "epoch: 12 step: 400 Loss: 0.00990\n",
      "epoch: 12 step: 450 Loss: 0.00831\n",
      "epoch: 12 step: 500 Loss: 0.00804\n",
      "epoch: 12 step: 550 Loss: 0.01030\n",
      "epoch: 12 step: 600 Loss: 0.00800\n",
      "epoch: 12 step: 650 Loss: 0.00625\n",
      "epoch: 12 step: 700 Loss: 0.00585\n",
      "epoch: 12 step: 750 Loss: 0.00829\n",
      "epoch: 12 step: 800 Loss: 0.00864\n",
      "epoch: 12 step: 850 Loss: 0.00657\n",
      "epoch: 12 step: 900 Loss: 0.01118\n",
      "epoch: 12 step: 950 Loss: 0.01058\n",
      "epoch: 12 step: 1000 Loss: 0.00721\n",
      "epoch: 12 step: 1050 Loss: 0.00901\n",
      "epoch: 12 step: 1100 Loss: 0.01096\n",
      "epoch: 12 step: 1150 Loss: 0.01055\n",
      "epoch: 12 step: 1200 Loss: 0.00749\n",
      "epoch: 12 step: 1250 Loss: 0.00893\n",
      "epoch: 12 step: 1300 Loss: 0.00660\n",
      "epoch: 12 step: 1350 Loss: 0.00980\n",
      "epoch: 12 step: 1400 Loss: 0.01447\n",
      "epoch: 12 step: 1450 Loss: 0.00906\n",
      "epoch: 12 step: 1500 Loss: 0.00999\n",
      "epoch: 12 step: 1550 Loss: 0.00663\n",
      "epoch: 12 step: 1600 Loss: 0.00698\n",
      "================> epoch: 12 average Loss: 0.00808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8fe6cab51d4da1868c9a89a15330b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 0 Loss: 0.00578\n",
      "epoch: 13 step: 50 Loss: 0.00542\n",
      "epoch: 13 step: 100 Loss: 0.00660\n",
      "epoch: 13 step: 150 Loss: 0.00813\n",
      "epoch: 13 step: 200 Loss: 0.01158\n",
      "epoch: 13 step: 250 Loss: 0.01148\n",
      "epoch: 13 step: 300 Loss: 0.00947\n",
      "epoch: 13 step: 350 Loss: 0.00893\n",
      "epoch: 13 step: 400 Loss: 0.00701\n",
      "epoch: 13 step: 450 Loss: 0.01463\n",
      "epoch: 13 step: 500 Loss: 0.00685\n",
      "epoch: 13 step: 550 Loss: 0.01006\n",
      "epoch: 13 step: 600 Loss: 0.00724\n",
      "epoch: 13 step: 650 Loss: 0.00969\n",
      "epoch: 13 step: 700 Loss: 0.00727\n",
      "epoch: 13 step: 750 Loss: 0.00609\n",
      "epoch: 13 step: 800 Loss: 0.01090\n",
      "epoch: 13 step: 850 Loss: 0.01217\n",
      "epoch: 13 step: 900 Loss: 0.00771\n",
      "epoch: 13 step: 950 Loss: 0.00833\n",
      "epoch: 13 step: 1000 Loss: 0.00840\n",
      "epoch: 13 step: 1050 Loss: 0.00552\n",
      "epoch: 13 step: 1100 Loss: 0.00645\n",
      "epoch: 13 step: 1150 Loss: 0.00787\n",
      "epoch: 13 step: 1200 Loss: 0.00637\n",
      "epoch: 13 step: 1250 Loss: 0.01031\n",
      "epoch: 13 step: 1300 Loss: 0.01194\n",
      "epoch: 13 step: 1350 Loss: 0.00419\n",
      "epoch: 13 step: 1400 Loss: 0.00653\n",
      "epoch: 13 step: 1450 Loss: 0.00912\n",
      "epoch: 13 step: 1500 Loss: 0.00751\n",
      "epoch: 13 step: 1550 Loss: 0.00608\n",
      "epoch: 13 step: 1600 Loss: 0.00469\n",
      "================> epoch: 13 average Loss: 0.00807\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bc3960c0884486a6f0330cea3b7416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 0 Loss: 0.00503\n",
      "epoch: 14 step: 50 Loss: 0.00660\n",
      "epoch: 14 step: 100 Loss: 0.00633\n",
      "epoch: 14 step: 150 Loss: 0.00899\n",
      "epoch: 14 step: 200 Loss: 0.00844\n",
      "epoch: 14 step: 250 Loss: 0.00839\n",
      "epoch: 14 step: 300 Loss: 0.00862\n",
      "epoch: 14 step: 350 Loss: 0.00730\n",
      "epoch: 14 step: 400 Loss: 0.00792\n",
      "epoch: 14 step: 450 Loss: 0.00862\n",
      "epoch: 14 step: 500 Loss: 0.00726\n",
      "epoch: 14 step: 550 Loss: 0.00500\n",
      "epoch: 14 step: 600 Loss: 0.00831\n",
      "epoch: 14 step: 650 Loss: 0.01075\n",
      "epoch: 14 step: 700 Loss: 0.01180\n",
      "epoch: 14 step: 750 Loss: 0.00735\n",
      "epoch: 14 step: 800 Loss: 0.00972\n",
      "epoch: 14 step: 850 Loss: 0.00795\n",
      "epoch: 14 step: 900 Loss: 0.00574\n",
      "epoch: 14 step: 950 Loss: 0.00727\n",
      "epoch: 14 step: 1000 Loss: 0.00601\n",
      "epoch: 14 step: 1050 Loss: 0.01108\n",
      "epoch: 14 step: 1100 Loss: 0.00931\n",
      "epoch: 14 step: 1150 Loss: 0.00677\n",
      "epoch: 14 step: 1200 Loss: 0.00789\n",
      "epoch: 14 step: 1250 Loss: 0.00625\n",
      "epoch: 14 step: 1300 Loss: 0.00945\n",
      "epoch: 14 step: 1350 Loss: 0.00761\n",
      "epoch: 14 step: 1400 Loss: 0.00584\n",
      "epoch: 14 step: 1450 Loss: 0.01274\n",
      "epoch: 14 step: 1500 Loss: 0.00805\n",
      "epoch: 14 step: 1550 Loss: 0.00967\n",
      "epoch: 14 step: 1600 Loss: 0.01024\n",
      "================> epoch: 14 average Loss: 0.00796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6327352fafe4481580b74877dade4190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 0 Loss: 0.00569\n",
      "epoch: 15 step: 50 Loss: 0.00945\n",
      "epoch: 15 step: 100 Loss: 0.00668\n",
      "epoch: 15 step: 150 Loss: 0.00949\n",
      "epoch: 15 step: 200 Loss: 0.00727\n",
      "epoch: 15 step: 250 Loss: 0.00809\n",
      "epoch: 15 step: 300 Loss: 0.00901\n",
      "epoch: 15 step: 350 Loss: 0.00799\n",
      "epoch: 15 step: 400 Loss: 0.01004\n",
      "epoch: 15 step: 450 Loss: 0.00874\n",
      "epoch: 15 step: 500 Loss: 0.00814\n",
      "epoch: 15 step: 550 Loss: 0.01077\n",
      "epoch: 15 step: 600 Loss: 0.00515\n",
      "epoch: 15 step: 650 Loss: 0.00656\n",
      "epoch: 15 step: 700 Loss: 0.00920\n",
      "epoch: 15 step: 750 Loss: 0.00840\n",
      "epoch: 15 step: 800 Loss: 0.00932\n",
      "epoch: 15 step: 850 Loss: 0.00880\n",
      "epoch: 15 step: 900 Loss: 0.00655\n",
      "epoch: 15 step: 950 Loss: 0.00649\n",
      "epoch: 15 step: 1000 Loss: 0.00835\n",
      "epoch: 15 step: 1050 Loss: 0.01108\n",
      "epoch: 15 step: 1100 Loss: 0.00826\n",
      "epoch: 15 step: 1150 Loss: 0.00996\n",
      "epoch: 15 step: 1200 Loss: 0.00839\n",
      "epoch: 15 step: 1250 Loss: 0.00826\n",
      "epoch: 15 step: 1300 Loss: 0.00566\n",
      "epoch: 15 step: 1350 Loss: 0.00887\n",
      "epoch: 15 step: 1400 Loss: 0.00773\n",
      "epoch: 15 step: 1450 Loss: 0.00725\n",
      "epoch: 15 step: 1500 Loss: 0.00960\n",
      "epoch: 15 step: 1550 Loss: 0.00762\n",
      "epoch: 15 step: 1600 Loss: 0.00719\n",
      "================> epoch: 15 average Loss: 0.00799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b8a9651ef64838bf3876eca64ee1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 0 Loss: 0.01006\n",
      "epoch: 16 step: 50 Loss: 0.00909\n",
      "epoch: 16 step: 100 Loss: 0.00840\n",
      "epoch: 16 step: 150 Loss: 0.00783\n",
      "epoch: 16 step: 200 Loss: 0.00770\n",
      "epoch: 16 step: 250 Loss: 0.00829\n",
      "epoch: 16 step: 300 Loss: 0.00903\n",
      "epoch: 16 step: 350 Loss: 0.01018\n",
      "epoch: 16 step: 400 Loss: 0.00870\n",
      "epoch: 16 step: 450 Loss: 0.00974\n",
      "epoch: 16 step: 500 Loss: 0.01140\n",
      "epoch: 16 step: 550 Loss: 0.00989\n",
      "epoch: 16 step: 600 Loss: 0.00652\n",
      "epoch: 16 step: 650 Loss: 0.00854\n",
      "epoch: 16 step: 700 Loss: 0.01234\n",
      "epoch: 16 step: 750 Loss: 0.00706\n",
      "epoch: 16 step: 800 Loss: 0.00824\n",
      "epoch: 16 step: 850 Loss: 0.00812\n",
      "epoch: 16 step: 900 Loss: 0.00777\n",
      "epoch: 16 step: 950 Loss: 0.00732\n",
      "epoch: 16 step: 1000 Loss: 0.00578\n",
      "epoch: 16 step: 1050 Loss: 0.00525\n",
      "epoch: 16 step: 1100 Loss: 0.01209\n",
      "epoch: 16 step: 1150 Loss: 0.00630\n",
      "epoch: 16 step: 1200 Loss: 0.00873\n",
      "epoch: 16 step: 1250 Loss: 0.01041\n",
      "epoch: 16 step: 1300 Loss: 0.00983\n",
      "epoch: 16 step: 1350 Loss: 0.01057\n",
      "epoch: 16 step: 1400 Loss: 0.00935\n",
      "epoch: 16 step: 1450 Loss: 0.00755\n",
      "epoch: 16 step: 1500 Loss: 0.00838\n",
      "epoch: 16 step: 1550 Loss: 0.00961\n",
      "epoch: 16 step: 1600 Loss: 0.00659\n",
      "================> epoch: 16 average Loss: 0.00800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db23692544b42eeb625c0d452b5f01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 0 Loss: 0.00602\n",
      "epoch: 17 step: 50 Loss: 0.00908\n",
      "epoch: 17 step: 100 Loss: 0.00862\n",
      "epoch: 17 step: 150 Loss: 0.00658\n",
      "epoch: 17 step: 200 Loss: 0.00675\n",
      "epoch: 17 step: 250 Loss: 0.00617\n",
      "epoch: 17 step: 300 Loss: 0.00656\n",
      "epoch: 17 step: 350 Loss: 0.00865\n",
      "epoch: 17 step: 400 Loss: 0.00457\n",
      "epoch: 17 step: 450 Loss: 0.00910\n",
      "epoch: 17 step: 500 Loss: 0.00751\n",
      "epoch: 17 step: 550 Loss: 0.00836\n",
      "epoch: 17 step: 600 Loss: 0.00670\n",
      "epoch: 17 step: 650 Loss: 0.00730\n",
      "epoch: 17 step: 700 Loss: 0.00542\n",
      "epoch: 17 step: 750 Loss: 0.00879\n",
      "epoch: 17 step: 800 Loss: 0.00568\n",
      "epoch: 17 step: 850 Loss: 0.00810\n",
      "epoch: 17 step: 900 Loss: 0.00640\n",
      "epoch: 17 step: 950 Loss: 0.00975\n",
      "epoch: 17 step: 1000 Loss: 0.00725\n",
      "epoch: 17 step: 1050 Loss: 0.00634\n",
      "epoch: 17 step: 1100 Loss: 0.00710\n",
      "epoch: 17 step: 1150 Loss: 0.00693\n",
      "epoch: 17 step: 1200 Loss: 0.00874\n",
      "epoch: 17 step: 1250 Loss: 0.01031\n",
      "epoch: 17 step: 1300 Loss: 0.00666\n",
      "epoch: 17 step: 1350 Loss: 0.00695\n",
      "epoch: 17 step: 1400 Loss: 0.00831\n",
      "epoch: 17 step: 1450 Loss: 0.00894\n",
      "epoch: 17 step: 1500 Loss: 0.00709\n",
      "epoch: 17 step: 1550 Loss: 0.00672\n",
      "epoch: 17 step: 1600 Loss: 0.00429\n",
      "================> epoch: 17 average Loss: 0.00795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f7f73ee08f4e4bb7b85e2f04748e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 0 Loss: 0.00903\n",
      "epoch: 18 step: 50 Loss: 0.00870\n",
      "epoch: 18 step: 100 Loss: 0.00785\n",
      "epoch: 18 step: 150 Loss: 0.00883\n",
      "epoch: 18 step: 200 Loss: 0.00800\n",
      "epoch: 18 step: 250 Loss: 0.00520\n",
      "epoch: 18 step: 300 Loss: 0.00543\n",
      "epoch: 18 step: 350 Loss: 0.01014\n",
      "epoch: 18 step: 400 Loss: 0.00908\n",
      "epoch: 18 step: 450 Loss: 0.00456\n",
      "epoch: 18 step: 500 Loss: 0.00968\n",
      "epoch: 18 step: 550 Loss: 0.00688\n",
      "epoch: 18 step: 600 Loss: 0.00670\n",
      "epoch: 18 step: 650 Loss: 0.00922\n",
      "epoch: 18 step: 700 Loss: 0.00770\n",
      "epoch: 18 step: 750 Loss: 0.00959\n",
      "epoch: 18 step: 800 Loss: 0.00870\n",
      "epoch: 18 step: 850 Loss: 0.00630\n",
      "epoch: 18 step: 900 Loss: 0.00981\n",
      "epoch: 18 step: 950 Loss: 0.00964\n",
      "epoch: 18 step: 1000 Loss: 0.00691\n",
      "epoch: 18 step: 1050 Loss: 0.00638\n",
      "epoch: 18 step: 1100 Loss: 0.00411\n",
      "epoch: 18 step: 1150 Loss: 0.00550\n",
      "epoch: 18 step: 1200 Loss: 0.00819\n",
      "epoch: 18 step: 1250 Loss: 0.00746\n",
      "epoch: 18 step: 1300 Loss: 0.00824\n",
      "epoch: 18 step: 1350 Loss: 0.00848\n",
      "epoch: 18 step: 1400 Loss: 0.00809\n",
      "epoch: 18 step: 1450 Loss: 0.00575\n",
      "epoch: 18 step: 1500 Loss: 0.01050\n",
      "epoch: 18 step: 1550 Loss: 0.00851\n",
      "epoch: 18 step: 1600 Loss: 0.00964\n",
      "================> epoch: 18 average Loss: 0.00797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67abd6cbed54204a561c357f3e70cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 0 Loss: 0.00815\n",
      "epoch: 19 step: 50 Loss: 0.00800\n",
      "epoch: 19 step: 100 Loss: 0.00895\n",
      "epoch: 19 step: 150 Loss: 0.00966\n",
      "epoch: 19 step: 200 Loss: 0.00643\n",
      "epoch: 19 step: 250 Loss: 0.00801\n",
      "epoch: 19 step: 300 Loss: 0.00682\n",
      "epoch: 19 step: 350 Loss: 0.00536\n",
      "epoch: 19 step: 400 Loss: 0.00859\n",
      "epoch: 19 step: 450 Loss: 0.00748\n",
      "epoch: 19 step: 500 Loss: 0.00919\n",
      "epoch: 19 step: 550 Loss: 0.00945\n",
      "epoch: 19 step: 600 Loss: 0.00877\n",
      "epoch: 19 step: 650 Loss: 0.00612\n",
      "epoch: 19 step: 700 Loss: 0.00983\n",
      "epoch: 19 step: 750 Loss: 0.00772\n",
      "epoch: 19 step: 800 Loss: 0.00714\n",
      "epoch: 19 step: 850 Loss: 0.01009\n",
      "epoch: 19 step: 900 Loss: 0.00598\n",
      "epoch: 19 step: 950 Loss: 0.00735\n",
      "epoch: 19 step: 1000 Loss: 0.00592\n",
      "epoch: 19 step: 1050 Loss: 0.00825\n",
      "epoch: 19 step: 1100 Loss: 0.00887\n",
      "epoch: 19 step: 1150 Loss: 0.00645\n",
      "epoch: 19 step: 1200 Loss: 0.00812\n",
      "epoch: 19 step: 1250 Loss: 0.00751\n",
      "epoch: 19 step: 1300 Loss: 0.00807\n",
      "epoch: 19 step: 1350 Loss: 0.00788\n",
      "epoch: 19 step: 1400 Loss: 0.00764\n",
      "epoch: 19 step: 1450 Loss: 0.00719\n",
      "epoch: 19 step: 1500 Loss: 0.00931\n",
      "epoch: 19 step: 1550 Loss: 0.00821\n",
      "epoch: 19 step: 1600 Loss: 0.00790\n",
      "================> epoch: 19 average Loss: 0.00783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f627afae5cce422c99a7943219f8850a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_hist = {}\n",
    "train_hist['losses'] = []\n",
    "print(\"start training\")\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        optimizer.param_groups[0]['lr'] /= 2\n",
    "    train(model, optimizer, epoch, train_hist)\n",
    "    \n",
    "    # save generated images\n",
    "    show_result(model, epoch)\n",
    "\n",
    "show_train_hist(train_hist, False, True, results_folder / 'history_train_losses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/ddpm_celeba.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "\n",
    "reverse_transform = Compose([\n",
    "    Lambda(lambda t: (t + 1) / 2),\n",
    "    Lambda(lambda t: t.permute(1, 2, 0)),  # CHW to HWC\n",
    "    Lambda(lambda t: t * 255.),\n",
    "    Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "    ToPILImage(),\n",
    "])\n",
    "def get_noisy_image(x_start, t):\n",
    "    # add noise\n",
    "    x_noisy = q_sample(x_start, t=t)\n",
    "\n",
    "    # turn back into PIL image\n",
    "    noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in train_loader:\n",
    "    save_image(x.add(1).mul(0.5).cpu(), results_folder / 'raw.png', nrow=10)\n",
    "    x_noisy = q_sample(x, t=torch.tensor([300-1])).to(device)\n",
    "    save_image(x_noisy.add(1).mul(0.5).cpu(), results_folder / 'noisy.png', nrow=10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_noisy\n",
    "for i in reversed(range(300)):\n",
    "    img = p_sample(model, img, torch.full(\n",
    "        (100,), i, device=device, dtype=torch.long), i)\n",
    "save_image(img.add(1).mul(0.5), results_folder / 'reconstruction.png', nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b86521161a495ea790a5892db47aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_images_list = sample(model, image_size=image_size, batch_size=100, channels=channels)\n",
    "all_images = all_images_list[-1].add(1).mul(0.5)\n",
    "save_image(all_images, str(results_folder /\n",
    "            f'sample_random.png'), nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample_from(x_start, t, n):\n",
    "    img = x_start.repeat(n, 1, 1, 1)torch.tensor([]).repeat\n",
    "    save_image(img.add(1).mul(0.5).cpu(), results_folder / 'sample_start.png', nrow=torch.sqrt(torch.tensor(n)).int())\n",
    "    for i in tqdm(reversed(range(t)), total=t):\n",
    "        img = p_sample(model, img, torch.full(\n",
    "            (n,), i, device=device, dtype=torch.long), i)\n",
    "    save_image(img.add(1).mul(0.5).cpu(), results_folder / 'sample_end.png', nrow=torch.sqrt(torch.tensor(n)).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201314ba8432463197312038240a6434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = 400\n",
    "n = 16\n",
    "for x, y in train_dataset:\n",
    "    if y[15] == 1:\n",
    "        save_image(x.add(1).mul(0.5).cpu(), results_folder / 'sample_real.png')\n",
    "        x_start = q_sample(x, torch.tensor([t-1])).to(device)\n",
    "        p_sample_from(x_start, t, n)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset: \n",
    "    x = x.unsqueeze(0)\n",
    "    print(x.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
